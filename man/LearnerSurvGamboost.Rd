% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerSurvGamboost.R
\name{LearnerSurvGamboost}
\alias{LearnerSurvGamboost}
\alias{mlr_learners_surv.gamboost}
\title{Gradient Boosting for Additive Models Survival Learner}
\format{\code{\link[R6:R6Class]{R6::R6Class()}} inheriting from \link{LearnerSurv}.}
\description{
Gradient boosting for optimizing arbitrary loss functions, where component-wise arbitrary
base-learners, e.g., smoothing procedures, are utilized as additive base-learners.
Calls \code{\link[mboost:gamboost]{mboost::gamboost()}} from package \CRANpkg{mboost}.
}
\details{
The \code{distr} return type is composed by using\code{\link[mboost:survFit]{mboost::survFit()}} which assumes a PH fit with
a Breslow estimator.
The \code{crank} return type is defined by the expectation of the survival distribution. \cr
The \code{lp} return type is given by \code{\link[mboost:predict.mboost]{mboost::predict.mboost()}}.

If the value given to the \code{Family} parameter is "custom.family" then an object of class
\code{\link[mboost:Family]{mboost::Family()}} needs to be passed to the \code{custom.family} parameter.

The only difference between \link{LearnerSurvGamboost} and \link{LearnerSurvMboost} is that the latter function
allows one to specify default degrees of freedom for smooth effects specified via
\code{baselearner = "bbs"}. In all other cases, degrees of freedom need to be set manually via a
specific definition of the corresponding base-learner.
}
\section{Construction}{
\preformatted{LearnerSurvGamboost$new()
mlr_learners$get("surv.gamboost")
lrn("surv.gamboost")
}
}

\examples{
library(mlr3)
task = tsk("rats")
learner = lrn("surv.gamboost")
learner$param_set$values = list(dfbase = 3, center = TRUE, baselearner = "bols")
resampling = rsmp("cv", folds = 3)
resample(task, learner, resampling)
}
\references{
Peter Buehlmann and Bin Yu (2003), Boosting with the L2 loss: regression and classification.
Journal of the American Statistical Association, 98, 324–339.

Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
Statistical Science, 22(4), 477–505.

Thomas Kneib, Torsten Hothorn and Gerhard Tutz (2009),
Variable selection and model choice in geoadditive regression models,
Biometrics, 65(2), 626–634.

Matthias Schmid and Torsten Hothorn (2008),
Boosting additive models using component-wise P-splines as base-learners.
Computational Statistics \& Data Analysis, 53(2), 298–311.

Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Mattthias Schmid and Benjamin Hofner (2010),
Model-based Boosting 2.0. Journal of Machine Learning Research, 11, 2109 – 2113.

Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid (2014).
Model-based Boosting in R: A Hands-on Tutorial Using the R Package mboost.
Computational Statistics, 29, 3–35.
\doi{10.1007/s00180-012-0382-5}
}
\seealso{
\link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}
}
